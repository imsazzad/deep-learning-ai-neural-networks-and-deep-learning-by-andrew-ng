
W_l = weith of layer l
b_l = bias of layer l

n_l = output neuron/unit of layer l => number of of neuron in layer l
n_l - 1 = same as above just previous layer

dW_l  = gradient of weight of layer l = > rate of chnage of final output with respect to W[l]
**************dW has the same shape as W => W.shape == dW.shape *****************

db_l  = gradient of bias of layer l = > rate of chnage of final output with respect to b[l]
**************db has the same shape as b => b.shape == db.shape *****************



				W_l.shape ===> n_l, n_(l-1) ;
				b_l.shape ===> n_l, 1 ;

				dW_l.shape ===> n_l, n_(l-1) ;
				db_l.shape ===> n_l, 1 ;


				z_l = output of layer l;
				z_l = output of layer l with activation;

				z_l.shape = a_l.shape = n_l, 1  ;
				Z_l.shape = A_l.shape = n_l, m  ;  // m is the number of training examples

				l = 0,   A_0 = X  => X.shape = (n_0, m);


				Z_l.shape = dZ.shape = A.shape = dA.shape

				dZ_l = dA_l  shape =>  (n_l, m); 